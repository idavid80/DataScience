# What is Data Science?

## Introducción a la Ciencia de Datos

La **Ciencia de Datos** es el proceso de analizar grandes cantidades de datos con el fin de encontrar patrones e ideas que ayuden a las organizaciones a tomar decisiones más informadas. Esta disciplina combina herramientas de **estadística**, **programación** y **análisis de datos** para extraer información valiosa de conjuntos de datos complejos y diversos, lo que puede ser crucial para tomar decisiones estratégicas en diversos sectores.

## Rutas hacia una Carrera en Ciencia de Datos

Existen muchas formas de empezar una carrera en Ciencia de Datos. Aunque muchos roles en esta área requieren habilidades en **matemáticas**, **programación** y una gran curiosidad por los datos, no todos los puestos requieren conocimientos técnicos avanzados. Algunos se enfocan más en la interpretación de los resultados y en la toma de decisiones basadas en los datos.

Para tener éxito como científico de datos, es importante que las personas interesadas en esta área sean:
- **Curiosos**: Estar dispuestos a explorar datos, cuestionar resultados y buscar patrones nuevos.
- **Críticos**: Saber juzgar la calidad de los datos y cómo estos pueden ser útiles para resolver problemas.
- **Argumentativos**: Ser capaces de defender sus ideas y explicar de manera clara y convincente lo que los datos les están diciendo.

## Demanda de Científicos de Datos

Los científicos de datos son muy buscados en el mercado laboral actual. Gracias a su capacidad para extraer información valiosa de los datos, los científicos de datos reciben **salarios altos** y tienen oportunidades de trabajar en una gran variedad de industrias, como la **financiera**, **tecnológica**, **salud**, **marketing**, entre otras.

## Un Día Típico en la Vida de un Científico de Datos

El trabajo de un científico de datos puede variar mucho según el tipo de proyecto en el que esté involucrado. Algunas de las tareas comunes que realiza un científico de datos incluyen:
- **Limpiar y preparar los datos**: Asegurarse de que los datos sean precisos y estén listos para el análisis.
- **Explorar y analizar patrones**: Buscar tendencias, relaciones y valores significativos dentro de los datos.
- **Desarrollar y probar modelos**: Utilizar algoritmos para hacer predicciones o clasificaciones basadas en los datos.
- **Comunicar resultados**: Explicar los hallazgos y hacer recomendaciones basadas en los análisis realizados.

## Algoritmos y Herramientas Clave en Ciencia de Datos

La ciencia de datos utiliza muchos **algoritmos** y técnicas para obtener información útil de los datos. Algunos algoritmos se enfocan en **clasificar** o **predecir** datos, mientras que otros buscan identificar **patrones** o **anomalías**. Estos algoritmos pueden ser muy complejos, pero permiten que los científicos de datos extraigan conclusiones importantes que pueden ayudar a las organizaciones a tomar decisiones informadas.

### Términos Clave en Ciencia de Datos

Aquí tienes algunos de los términos más comunes que se utilizan en la ciencia de datos:

- **Outliers (Valores Atípicos)**: Son datos que se alejan mucho de los demás, lo que puede afectar los resultados del análisis. Identificarlos puede ser crucial para mejorar la calidad de los datos.
- **Modelos**: Son representaciones matemáticas que ayudan a explicar o predecir patrones que existen en los datos. Los modelos permiten hacer suposiciones basadas en la información disponible.
- **Algoritmos**: Son secuencias de pasos lógicos que ayudan a procesar los datos y obtener **insights** o conclusiones valiosas.
- **JSON (JavaScript Object Notation)**: Es un formato de datos que se usa para intercambiar información entre diferentes aplicaciones de forma ligera y fácil de entender.
- **XML (Extensible Markup Language)**: Es un formato de datos que organiza la información de forma jerárquica, y se usa mucho para la transferencia de datos a través de la web.
- **CSV (Comma-Separated Values)**: Es un formato simple para almacenar datos en tablas, donde los valores se separan por comas. Es uno de los formatos más comunes para manejar grandes volúmenes de datos en archivos.
- **Regresión**: Es una técnica estadística que se usa para entender la relación entre dos o más variables. Se utiliza principalmente para hacer **predicciones** sobre datos futuros basándose en patrones pasados.



## Glosario de lección sobre Big Data y Minería de Datos

| **Término**                      | **Definición**                                                                                                                                                                          | **Video donde se presenta el término**                      |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|
| **Analytics (Análisis de Datos)** | El proceso de examinar datos para extraer conclusiones y tomar decisiones informadas. Es un aspecto fundamental de la ciencia de datos que involucra análisis estadístico e información basada en datos. | Data Scientists at New York University                     |
| **Big Data (Gran Volumen de Datos)** | Grandes cantidades de datos que incluyen datos estructurados, semi-estructurados y no estructurados. Estos datos son valiosos cuando se analizan, ya que pueden proporcionar ventajas competitivas. | How Big Data is Driving Digital Transformation             |
| **Big Data Cluster (Clúster de Big Data)** | Un entorno informático distribuido que comprende miles o decenas de miles de computadoras interconectadas que almacenan y procesan grandes conjuntos de datos.                            | What is Hadoop?                                            |
| **Broad Network Access (Acceso Amplio a la Red)** | La capacidad de acceder a los recursos en la nube mediante plataformas y dispositivos estándar, como dispositivos móviles, laptops y estaciones de trabajo a través de redes.            | Introduction to Cloud                                      |
| **Chief Data Officer (CDO)**      | Un rol emergente encargado de supervisar iniciativas relacionadas con los datos, la gobernanza y las estrategias, asegurando que los datos jueguen un papel central en la transformación digital. | How Big Data is Driving Digital Transformation             |
| **Chief Information Officer (CIO)** | Un ejecutivo responsable de gestionar la tecnología de la información y los sistemas informáticos de una organización, contribuyendo a los aspectos tecnológicos de la transformación digital. | How Big Data is Driving Digital Transformation             |
| **Cloud Computing (Computación en la Nube)** | El suministro de recursos informáticos bajo demanda, como redes, servidores, almacenamiento, aplicaciones y centros de datos, a través de Internet, bajo un modelo de pago por uso.     | Introduction to Cloud                                      |
| **Cloud Deployment Models (Modelos de Implementación en la Nube)** | Categorías que indican dónde residen la infraestructura en la nube, quién la gestiona y cómo los recursos y servicios en la nube están disponibles para los usuarios, incluyendo modelos públicos, privados e híbridos. | Introduction to Cloud                                      |
| **Cloud Service Models (Modelos de Servicios en la Nube)** | Modelos basados en las capas de una pila de computación, como Infraestructura como Servicio (IaaS), Plataforma como Servicio (PaaS) y Software como Servicio (SaaS), que representan diferentes ofertas de computación en la nube. | Introduction to Cloud                                      |
| **Commodity Hardware (Hardware Estándar)** | Componentes de hardware estándar y económicos usados en un clúster de Big Data, lo que ofrece soluciones rentables para almacenamiento y procesamiento sin depender de hardware especializado. | What is Hadoop?                                            |
| **Data Algorithms (Algoritmos de Datos)** | Procedimientos computacionales y modelos matemáticos usados para procesar y analizar datos disponibles en la nube, que los científicos de datos utilizan para procesar grandes conjuntos de datos de manera eficiente. | Cloud for Data Science                                      |
| **Data Replication (Replicación de Datos)** | Estrategia en la que los datos se duplican a través de múltiples nodos en un clúster para asegurar la durabilidad y disponibilidad de los datos, reduciendo el riesgo de pérdida de datos por fallas de hardware. | What is Hadoop?                                            |
| **Data Science (Ciencia de Datos)** | Un campo interdisciplinario que implica extraer información y conocimiento de los datos utilizando diversas técnicas, como programación, estadísticas y herramientas analíticas.          | Data Scientists at New York University                     |
| **Deep Learning (Aprendizaje Profundo)** | Un subconjunto de machine learning que involucra redes neuronales artificiales inspiradas en el cerebro humano, capaces de aprender y tomar decisiones complejas de manera autónoma.      | Data Scientists at New York University                     |
| **Digital Change (Cambio Digital)** | La integración de la tecnología digital en los procesos y operaciones comerciales que lleva a mejoras e innovaciones en la manera en que las organizaciones operan y entregan valor a los clientes. | How Big Data is Driving Digital Transformation             |
| **Digital Transformation (Transformación Digital)** | Un cambio estratégico y cultural organizacional impulsado por la ciencia de datos, especialmente Big Data, que integra tecnología digital en todas las áreas de la organización, resultando en cambios fundamentales operativos y de entrega de valor. | How Big Data is Driving Digital Transformation             |
| **Distributed Data (Datos Distribuidos)** | La práctica de dividir los datos en partes más pequeñas y distribuirlas a través de múltiples computadoras dentro de un clúster para permitir el procesamiento paralelo en el análisis de datos. | What is Hadoop?                                            |
| **Hadoop**                        | Un marco de almacenamiento y procesamiento distribuido utilizado para manejar y analizar grandes conjuntos de datos. Es particularmente adecuado para el análisis de Big Data y aplicaciones de ciencia de datos. | Data Scientists at New York University                     |
| **Hadoop Distributed File System (HDFS)** | Un sistema de almacenamiento dentro del marco de Hadoop que divide y distribuye archivos a través de múltiples nodos, facilitando el acceso paralelo a los datos y la tolerancia a fallos. | What is Hadoop?                                            |
| **Infrastructure as a Service (IaaS)** | Un modelo de servicio en la nube que proporciona acceso a infraestructura de computación, como servidores, almacenamiento y redes, sin necesidad de que los usuarios gestionen o operen estos recursos. | Introduction to Cloud                                      |
| **Java-Based Framework (Marco Basado en Java)** | Hadoop está implementado en Java, un lenguaje de programación de alto nivel y de código abierto que proporciona la base para construir soluciones de almacenamiento y procesamiento distribuido. | Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark  |
| **Map Process (Proceso Map)**      | El primer paso en el modelo de programación MapReduce de Hadoop, donde los datos se procesan en paralelo en los nodos del clúster. Se utiliza comúnmente en tareas de transformación de datos. | What is Hadoop?                                            |
| **Measured Service (Servicio Medido)** | Característica en la que los usuarios son facturados por los recursos en la nube según su uso real, con la utilización de recursos monitoreada, medida e informada de manera transparente. | Introduction to Cloud                                      |
| **On-Demand Self-Service (Autoservicio Bajo Demanda)** | La capacidad para que los usuarios accedan y provisionen recursos en la nube, como poder de procesamiento, almacenamiento y redes, utilizando interfaces simples sin interacción humana con los proveedores de servicios. | Introduction to Cloud                                      |
| **Rapid Elasticity (Elasticidad Rápida)** | La capacidad de escalar rápidamente los recursos en la nube hacia arriba o hacia abajo según la demanda, lo que permite a los usuarios acceder a más recursos cuando los necesiten y liberarlos cuando no los estén utilizando. | Introduction to Cloud                                      |
| **Reduce Process (Proceso Reduce)** | El segundo paso en el modelo MapReduce de Hadoop, donde los resultados del proceso Map se agregan y se procesan más para producir el resultado final, generalmente usado para análisis. | What is Hadoop?                                            |
| **Replication (Replicación)**      | La creación de copias de partes de los datos dentro de un clúster de Big Data para mejorar la tolerancia a fallos y asegurar la disponibilidad de los datos en caso de fallas de hardware o nodos. | What is Hadoop?                                            |
| **Resource Pooling (Agrupación de Recursos)** | Característica de la nube donde los recursos de computación se comparten y asignan dinámicamente a múltiples consumidores, promoviendo economías de escala y eficiencia en costos. | Introduction to Cloud                                      |
| **Skills Network Labs (SN Labs)** | Recursos de aprendizaje proporcionados por IBM, que incluyen herramientas como Jupyter Notebooks y clústeres de Spark, disponibles para que los estudiantes trabajen en proyectos de ciencia de datos en la nube y desarrollen habilidades. | Cloud for Data Science                                      |
| **Spilling to Disk (Desbordamiento a Disco)** | Técnica utilizada en situaciones donde la memoria es limitada, en la que los datos se escriben temporalmente en el almacenamiento en disco cuando los recursos de memoria se agotan, asegurando que el procesamiento continúe sin interrupciones. | Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark  |
| **STEM Classes (Clases STEM)**     | Cursos de Ciencia, Tecnología, Ingeniería y Matemáticas (STEM) generalmente impartidos en las escuelas secundarias para preparar a los estudiantes para carreras técnicas, incluyendo la ciencia de datos. | Data Scientists at New York University                     |
| **Variety (Variedad)**             | La diversidad de tipos de datos, incluyendo datos estructurados y no estructurados provenientes de diversas fuentes, como texto, imágenes, video, entre otros, lo que plantea desafíos en la gestión de datos. | Foundations of Big Data                                    |
| **Velocity (Velocidad)**          | La velocidad con la que los datos se acumulan y se generan, a menudo en tiempo real o casi en tiempo real, lo que impulsa la necesidad de un procesamiento y análisis rápidos de los datos. | Foundations of Big Data                                    |
| **Veracity (Veracidad)**          | La calidad y precisión de los datos, asegurando que se ajusten a los hechos, sean consist

# Glosario de Términos de Ciencia de Datos

Este glosario contiene los términos clave de esta lección. Estos términos son importantes para reconocer cuando se trabaja en la industria, participando en grupos de usuarios o en otros programas de certificación.

| **Término**                             | **Definición**                                                                                                                                                    | **Video donde se introduce el término**                                 |
|-----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------|
| **Cumplimiento ACID**                   | Asegurar la exactitud y consistencia de los datos a través de las propiedades de Atomicidad, Consistencia, Aislamiento y Durabilidad (ACID) en las transacciones.  | *Sistema de Gestión de Bases de Datos Relacionales*                    |
| **Plataforma de Integración basada en la nube (iPaaS)** | Plataformas de integración alojadas en la nube que ofrecen servicios de integración a través de nubes privadas virtuales o modelos de nube híbrida, proporcionando escalabilidad y flexibilidad. | *Plataformas de Integración de Datos*                                  |
| **Base de Datos Basada en Columnas**    | Tipo de base de datos NoSQL que organiza los datos en celdas agrupadas como columnas, utilizada en sistemas que requieren un alto volumen de solicitudes de escritura y almacenamiento de datos de series de tiempo o IoT. | *NoSQL*                                                              |
| **Datos en reposo**                     | Datos que están almacenados y no están en movimiento activo, generalmente en bases de datos o sistemas de almacenamiento para diversos fines, como respaldos.     | *Consideraciones para la Elección de Repositorios de Datos*            |
| **Integración de Datos**                | Disciplina que involucra prácticas y técnicas arquitectónicas que permiten a las organizaciones ingerir, transformar, combinar y provisionar datos a través de diversos tipos de datos. | *Plataformas de Integración de Datos*                                  |
| **Data Lake (Lago de Datos)**           | Repositorio de datos que almacena grandes volúmenes de datos estructurados, semi-estructurados y no estructurados en su formato nativo, facilitando la exploración y el análisis de datos. | *Lagos de Datos, Data Marts, ETL y Pipelines de Datos*                 |
| **Data Mart (Mercado de Datos)**        | Subconjunto de un almacén de datos diseñado para funciones comerciales específicas o comunidades de usuarios, proporcionando seguridad aislada y rendimiento para análisis enfocados. | *Lagos de Datos, Data Marts, ETL y Pipelines de Datos*                 |
| **Pipeline de Datos**                   | Proceso integral de movimiento de datos que cubre todo el recorrido de los datos desde los sistemas de origen hasta los sistemas de destino, e incluye la integración de datos como componente clave. | *Plataformas de Integración de Datos*                                  |
| **Repositorio de Datos**                | Término general que hace referencia a datos que han sido recolectados, organizados y aislados para operaciones comerciales o análisis de datos. Puede incluir bases de datos, almacenes de datos y repositorios de big data. | *Recolección y Organización de Datos*                                  |
| **Almacén de Datos (Data Warehouse)**   | Repositorio central que consolida datos de diversas fuentes mediante el proceso Extract, Transform, Load (ETL), facilitando el acceso para análisis e inteligencia empresarial. | *Recolección y Organización de Datos*                                  |
| **Base de Datos Documental**            | Tipo de base de datos NoSQL que almacena cada registro y sus datos asociados dentro de un único documento, permitiendo la indexación flexible, consultas ad hoc y análisis sobre colecciones de documentos. | *NoSQL*                                                              |
| **Proceso ETL**                         | Proceso de extracción, transformación y carga para la integración de datos que implica extraer datos de diversas fuentes, transformarlos a un formato utilizable y cargarlos en un repositorio. | *Lagos de Datos, Data Marts, ETL y Pipelines de Datos*                 |
| **Base de Datos basada en Grafo**       | Tipo de base de datos NoSQL que utiliza un modelo gráfico para representar y almacenar datos, ideal para visualizar, analizar y descubrir conexiones entre puntos de datos interconectados. | *NoSQL*                                                              |
| **Almacén de Clave-Valor**              | Tipo de base de datos NoSQL donde los datos se almacenan como pares clave-valor, con la clave como identificador único y el valor que contiene los datos, los cuales pueden ser simples o complejos. | *NoSQL*                                                              |
| **Portabilidad**                        | Capacidad de las herramientas de integración de datos para ser usadas en diversos entornos, como nubes individuales, multi-nube o nubes híbridas, ofreciendo flexibilidad en las opciones de despliegue. | *Plataformas de Integración de Datos*                                  |
| **Conectores preconstruidos**           | Conectores y adaptadores catalogados que simplifican la conexión y construcción de flujos de integración con diversas fuentes de datos como bases de datos, archivos planos, redes sociales, APIs, aplicaciones CRM y ERP. | *Plataformas de Integración de Datos*                                  |
| **Bases de Datos Relacionales (RDBMS)** | Bases de datos que organizan los datos en formato tabular con filas y columnas, siguiendo una estructura y esquema bien definidos. | *Recolección y Organización de Datos*                                  |
| **Escalabilidad**                       | Capacidad de un repositorio de datos para crecer y expandir su capacidad para manejar volúmenes crecientes de datos y demandas de carga de trabajo a lo largo del tiempo. | *Consideraciones para la Elección de Repositorios de Datos*            |
| **Esquema**                             | Estructura predefinida que describe la organización y el formato de los datos dentro de una base de datos, indicando los tipos de datos permitidos y sus relaciones. | *Consideraciones para la Elección de Repositorios de Datos*            |
| **Datos en tiempo real**                | Datos que se generan y transmiten de forma continua en tiempo real, lo que requiere un manejo y procesamiento especializado para capturarlos y analizarlos. | *Consideraciones para la Elección de Repositorios de Datos*            |
| **Casos de uso de bases de datos relacionales** | Aplicaciones como el procesamiento de transacciones en línea (OLTP), almacenes de datos (OLAP) y soluciones de IoT donde las bases de datos relacionales son más eficaces. | *Sistema de Gestión de Bases de Datos Relacionales*                    |
| **Bloqueo por proveedor**               | Situación en la que un usuario se vuelve dependiente de las tecnologías y soluciones de un proveedor específico, lo que hace difícil cambiar a otras plataformas. | *Plataformas de Integración de Datos*                                  |

## Conclusiones

1. **Las cinco características de los macrodatos**  
   Los macrodatos o **Big Data** se definen por cinco características esenciales:  
   - **Velocidad**: La rapidez con la que se generan y procesan los datos.  
   - **Volumen**: La cantidad masiva de datos que se almacenan y analizan.  
   - **Variedad**: Los diferentes tipos de datos (estructurados, semi-estructurados y no estructurados).  
   - **Veracidad**: La calidad y fiabilidad de los datos.  
   - **Valor**: La importancia de extraer información útil de los datos.  
   Estas características determinan cómo se manejan, procesan y utilizan los datos en el mundo actual.

2. **Las cinco características de la computación en la nube**  
   La **computación en la nube** también tiene cinco características clave que facilitan el acceso y uso de recursos tecnológicos de manera más eficiente:  
   - **Autoservicio a la carta**: Los usuarios pueden acceder a los recursos cuando los necesiten, sin intervención directa del proveedor.  
   - **Amplio acceso a la red**: La capacidad de acceder a recursos desde cualquier lugar a través de Internet.  
   - **Puesta en común de recursos**: Los recursos son compartidos entre muchos usuarios para maximizar su eficiencia.  
   - **Rápida elasticidad**: La habilidad de ajustar los recursos rápidamente según la demanda.  
   - **Servicio medido**: Los usuarios pagan solo por lo que usan, lo que hace el servicio más económico y flexible.

3. **El proceso de la minería de datos**  
   La **minería de datos** es un proceso crucial para descubrir patrones en grandes volúmenes de datos. Consiste en seis pasos fundamentales:  
   - **Fijación de objetivos**: Determinar qué se espera lograr con el análisis de datos.  
   - **Selección de fuentes de datos**: Elegir los datos adecuados para el análisis.  
   - **Preprocesamiento**: Limpiar y preparar los datos para su análisis.  
   - **Transformación**: Convertir los datos a un formato adecuado.  
   - **Minería**: Aplicar algoritmos para encontrar patrones en los datos.  
   - **Evaluación**: Valorar los resultados obtenidos y determinar si cumplen los objetivos planteados.

4. **La importancia de la tecnología escalable**  
   Debido a la enorme cantidad y diversidad de los datos generados por personas, herramientas y máquinas, es necesario utilizar tecnologías **nuevas, innovadoras y escalables** para procesar y aprovechar estos datos. La transformación digital depende de estas herramientas para manejar y analizar datos de manera eficiente.

5. **El aprendizaje profundo y el aprendizaje automático**  
   - **Aprendizaje profundo**: Es un tipo de **inteligencia artificial (IA)** que utiliza redes neuronales para "aprender" patrones de datos, es decir, autoenseñarse mediante entradas y salidas.  
   - **Aprendizaje automático**: Es una subdisciplina de la IA que permite a las máquinas aprender de los datos y hacer **predicciones** sin necesidad de ser programadas explícitamente para cada tarea. Ambos métodos son esenciales para el análisis de grandes volúmenes de datos.

6. **La regresión y su importancia**  
   La **regresión** es una técnica estadística utilizada para **identificar** y medir la relación entre variables. Nos ayuda a entender cómo una o más entradas afectan a una salida, lo cual es clave para hacer predicciones y decisiones basadas en datos.

7. **Conocimientos necesarios para procesar Big Data**  
   Para trabajar con **Big Data**, se requieren conocimientos de **estadística**, **modelos de aprendizaje automático** y algo de **programación**. Estos conocimientos son fundamentales para entender los patrones en los datos y aplicar técnicas de análisis de manera eficiente.

8. **La inteligencia artificial generativa**  
   La **IA generativa** es una rama de la inteligencia artificial que se enfoca en **crear** nuevos datos, en lugar de solo analizar los existentes. Esta tecnología permite a las máquinas **generar contenido original**, como imágenes, música, lenguaje, e incluso código informático, imitando las creaciones humanas y abriendo un mundo de posibilidades en la automatización y creatividad.

