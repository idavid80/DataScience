# Machine Learning with Python

## Introducci√≥n a la Regresi√≥n y Evaluaci√≥n de Modelos

### 1. ¬øQu√© es la Regresi√≥n?
La regresi√≥n es una t√©cnica estad√≠stica utilizada para predecir valores continuos. En un contexto pr√°ctico, la regresi√≥n nos permite estimar valores, como la emisi√≥n de CO2 de un autom√≥vil, a partir de otras variables relacionadas, como el tama√±o del motor o el n√∫mero de cilindros.

En t√©rminos generales, la regresi√≥n implica dos tipos de variables:
- **Variable dependiente (Y)**: el valor que deseamos predecir, como la emisi√≥n de CO2.
- **Variables independientes (X)**: los valores explicativos, como el tama√±o del motor.

En resumen, la regresi√≥n busca modelar la relaci√≥n entre **Y** y una funci√≥n de **X**. La clave es que **Y** debe ser un valor continuo, mientras que las variables independientes **X** pueden ser categ√≥ricas o continuas.

#### Tipos de Regresi√≥n
- **Regresi√≥n simple**: se utiliza una sola variable independiente. Por ejemplo, predecir la emisi√≥n de CO2 en funci√≥n del tama√±o del motor.
- **Regresi√≥n m√∫ltiple**: se emplean m√∫ltiples variables independientes, como tama√±o del motor y n√∫mero de cilindros para estimar la emisi√≥n de CO2.

### 2. Aplicaciones Comunes de la Regresi√≥n
La regresi√≥n se utiliza en diversos campos, como:
- **Ventas**: Previsi√≥n de ventas anuales en funci√≥n de factores como edad, experiencia, etc.
- **Psicolog√≠a**: Evaluaci√≥n de la satisfacci√≥n individual a partir de factores demogr√°ficos.
- **Bienes ra√≠ces**: Estimaci√≥n del precio de una vivienda en funci√≥n de su tama√±o, n√∫mero de habitaciones, entre otros.

### 3. Introducci√≥n a la Regresi√≥n Lineal
La regresi√≥n lineal es el modelo m√°s b√°sico y com√∫n para describir la relaci√≥n entre dos o m√°s variables. Existen dos tipos principales de modelos:
- **Regresi√≥n lineal simple**: una variable independiente (**X**) predice la variable dependiente (**Y**).
- **Regresi√≥n lineal m√∫ltiple**: varias variables independientes predicen una variable dependiente.

Para aplicar la regresi√≥n lineal, necesitamos:
- **Variable independiente (X)**: en este ejemplo, tama√±o del motor.
- **Variable dependiente (Y)**: emisi√≥n de CO2.

La regresi√≥n lineal busca encontrar la mejor l√≠nea de ajuste que pase a trav√©s de los datos para predecir **Y** a partir de **X**.

### C√≥mo Funciona la Regresi√≥n Lineal
Dado un conjunto de datos, la relaci√≥n entre la variable independiente (**X**) y la dependiente (**Y**) se representa como:

\[
y = \theta_0 + \theta_1 x
\]

Donde:
- **ùúÉ‚ÇÄ**: el intercepto (valor de Y cuando X = 0)
- **ùúÉ‚ÇÅ**: la pendiente de la l√≠nea o el coeficiente que determina c√≥mo cambia Y cuando X cambia.

El objetivo es minimizar la suma de errores (diferencia entre los valores observados y los valores predichos), com√∫nmente representada por el **Error Cuadr√°tico Medio (MSE)**.

#### Ejemplo Pr√°ctico
Supongamos que queremos predecir la emisi√≥n de CO2 para un coche con un tama√±o de motor de 2.4:

Si encontramos:
- **ùúÉ‚ÇÄ = 92.94**
- **ùúÉ‚ÇÅ = 43.98**

Entonces podemos usar la ecuaci√≥n de regresi√≥n para predecir:

\[
CO‚ÇÇ\ Emission = 92.94 + 43.98 \times 2.4
\]

### 4. Evaluaci√≥n de Modelos de Regresi√≥n
Evaluar el rendimiento del modelo es crucial para determinar su precisi√≥n. Hay dos enfoques comunes para la evaluaci√≥n:

#### 4.1 Entrenar y Probar en el Mismo Conjunto de Datos
Consiste en entrenar el modelo en todo el conjunto de datos y luego probarlo en una porci√≥n de ese mismo conjunto. Este m√©todo suele dar un alto nivel de precisi√≥n en el entrenamiento pero puede llevar a un modelo sobreajustado (alto error en datos desconocidos).

#### 4.2 Divisi√≥n de Entrenamiento y Prueba
Aqu√≠, el conjunto de datos se divide en dos subconjuntos:
- **Entrenamiento**: para construir el modelo.
- **Prueba**: para evaluar el rendimiento del modelo en datos no vistos.

Esta t√©cnica es preferida para evaluar la precisi√≥n en datos desconocidos, ya que el modelo no ha sido expuesto a los datos de prueba.

#### 4.3 Validaci√≥n Cruzada K-Fold (K-Fold Cross-Validation)
La validaci√≥n cruzada K-fold es una t√©cnica avanzada que reduce la variabilidad en los resultados. Aqu√≠, el conjunto de datos se divide en **K subconjuntos** o "folds":
- En cada iteraci√≥n, uno de los fold se usa como conjunto de prueba, y los restantes como conjunto de entrenamiento.
- Los resultados se promedian para obtener una medida de precisi√≥n m√°s estable y confiable.

Este m√©todo permite una evaluaci√≥n m√°s precisa y confiable del rendimiento del modelo.

### Conclusi√≥n
La regresi√≥n, especialmente la regresi√≥n lineal, es una herramienta valiosa para la predicci√≥n de valores continuos en m√∫ltiples disciplinas. Con una comprensi√≥n b√°sica de los principios de ajuste de modelos y t√©cnicas de evaluaci√≥n, es posible construir y evaluar modelos de regresi√≥n robustos.

---

## Introducci√≥n a la Clasificaci√≥n

En este conjunto de apuntes, vamos a explorar los conceptos fundamentales de la clasificaci√≥n en el √°mbito del aprendizaje autom√°tico (machine learning). La clasificaci√≥n es un enfoque de aprendizaje supervisado, lo que significa que se utiliza un conjunto de datos etiquetados (conocidos) para entrenar un modelo que posteriormente sea capaz de predecir o clasificar nuevos casos no etiquetados.

### ¬øQu√© es la clasificaci√≥n?

La clasificaci√≥n consiste en categorizar o clasificar elementos desconocidos en un conjunto discreto de clases. El objetivo es aprender la relaci√≥n entre un conjunto de variables caracter√≠sticas y una variable objetivo de inter√©s. Esta variable objetivo en la clasificaci√≥n es categ√≥rica, con valores discretos.

Por ejemplo, si tenemos un conjunto de datos sobre pr√©stamos y sus posibles impagos, la clasificaci√≥n puede usarse para predecir si un nuevo cliente es un "incumplidor" o "no incumplidor", bas√°ndose en caracter√≠sticas como la edad, ingresos, nivel educativo, etc.

### ¬øC√≥mo funciona la clasificaci√≥n?

La clasificaci√≥n funciona al entrenar un modelo con datos etiquetados (conocidos). Una vez que el modelo ha aprendido esta relaci√≥n, puede usarla para predecir la clase de nuevos datos no etiquetados (por ejemplo, determinar si un nuevo cliente ser√° un incumplidor).

Un buen ejemplo de clasificaci√≥n es la predicci√≥n de impagos de pr√©stamos. Si un banco tiene datos previos de clientes que no pagaron su pr√©stamo, puede usar estos datos para entrenar un clasificador y, cuando un nuevo cliente solicite un pr√©stamo, el banco puede predecir si es un "alto riesgo" o "bajo riesgo".

### Clasificaci√≥n binaria vs clasificaci√≥n multiclase

La clasificaci√≥n puede ser binaria, como en el ejemplo del pr√©stamo (¬øincumplidor o no?), o multiclase, como cuando se tienen varias categor√≠as. Un ejemplo de clasificaci√≥n multiclase es el de predecir a qu√© grupo de clientes pertenece una persona bas√°ndose en datos demogr√°ficos (edad, ingresos, etc.). Un banco podr√≠a usar este modelo para predecir el grupo de servicio m√°s adecuado para un nuevo cliente: servicio b√°sico, servicio electr√≥nico, servicio Plus o servicio total.

### Casos de uso de la clasificaci√≥n

La clasificaci√≥n tiene numerosas aplicaciones en los negocios y en diversas industrias. Algunos ejemplos incluyen:

- **Predicci√≥n de cancelaciones**: Identificar si un cliente abandonar√° el servicio de una empresa (churn detection).
- **Detecci√≥n de fraude**: Predecir si una transacci√≥n es fraudulenta o no.
- **Filtrado de correos electr√≥nicos**: Identificar si un correo es spam o no.
- **Reconocimiento de voz y escritura**: Clasificar el habla o la escritura a mano.

### Algoritmos de clasificaci√≥n

Existen varios algoritmos de clasificaci√≥n que se pueden utilizar dependiendo del problema. Algunos de los m√°s comunes son:

- √Årboles de decisi√≥n
- Naive Bayes
- An√°lisis discriminante lineal
- K-Vecinos m√°s cercanos (K-Nearest Neighbors, K-NN)
- Regresi√≥n log√≠stica
- Redes neuronales
- M√°quinas de soporte vectorial (SVM)

En este curso, solo cubriremos algunos de estos algoritmos, destacando el de **K-Vecinos m√°s cercanos (K-NN)**.

---

### K-Vecinos m√°s Cercanos (K-NN)

El algoritmo **K-Vecinos m√°s cercanos** es un enfoque popular para la clasificaci√≥n. Este algoritmo clasifica un punto de datos nuevo seg√∫n los puntos de datos m√°s cercanos en el conjunto de entrenamiento. 

#### ¬øC√≥mo funciona K-NN?

Imaginemos que una empresa de telecomunicaciones ha segmentado a sus clientes en cuatro grupos seg√∫n su patr√≥n de uso del servicio. La empresa quiere predecir a qu√© grupo pertenece un nuevo cliente bas√°ndose en su edad e ingresos.

1. **Selecci√≥n de K**: Se selecciona un valor para K (por ejemplo, 5), que representa la cantidad de vecinos m√°s cercanos que se analizar√°n.
2. **C√°lculo de distancias**: Se calcula la distancia entre el nuevo cliente y los puntos de datos existentes, utilizando una m√©trica como la distancia **euclidiana**.
3. **Clasificaci√≥n por mayor√≠a**: Los K vecinos m√°s cercanos al nuevo cliente determinan su clase. Si la mayor√≠a de los K vecinos pertenece a un grupo espec√≠fico, el nuevo cliente se clasifica en ese grupo.

El valor de **K** es crucial. Si K es muy peque√±o, el modelo puede sobreajustarse (overfitting) y si K es muy grande, el modelo puede ser demasiado general (underfitting). Se recomienda probar diferentes valores de K y evaluar cu√°l da el mejor rendimiento.

#### Elecci√≥n del valor de K

Elegir el valor correcto de K es esencial. Si seleccionamos un valor bajo, como K=1, el modelo podr√≠a ser muy sensible a los datos at√≠picos o ruido. Por otro lado, si elegimos un valor de K muy alto, el modelo podr√≠a volverse demasiado simple y no capturar las complejidades de los datos.

Para encontrar el mejor valor de K, se utiliza la **validaci√≥n cruzada**, dividiendo los datos en conjuntos de entrenamiento y prueba y evaluando la precisi√≥n del modelo con diferentes valores de K.

---

### M√©tricas de Evaluaci√≥n para Clasificadores

Las m√©tricas de evaluaci√≥n son fundamentales para medir el desempe√±o de un modelo de clasificaci√≥n. Algunas de las m√©tricas m√°s comunes son:

#### 1. √çndice de Jaccard

Mide la similitud entre dos conjuntos de etiquetas (verdaderas y predichas). Se calcula como la intersecci√≥n de los conjuntos dividido por su uni√≥n.

#### 2. Matriz de confusi√≥n

Es una herramienta que muestra las predicciones correctas y err√≥neas en relaci√≥n con las etiquetas verdaderas. De la matriz se pueden derivar otras m√©tricas, como:

- **Precisi√≥n**: Proporci√≥n de predicciones correctas entre todas las predicciones hechas.
- **Recuperaci√≥n**: Proporci√≥n de verdaderos positivos entre todos los casos que deber√≠an haber sido positivos.

#### 3. F1-Score

Es la media arm√≥nica entre la precisi√≥n y la recuperaci√≥n. Este valor es √∫til cuando hay un desequilibrio entre las clases.

#### 4. Log-Loss (P√©rdida Logar√≠tmica)

Mide el rendimiento de un clasificador cuando el resultado es una probabilidad entre 0 y 1. Cuanto menor sea el valor de log-loss, mejor es el modelo.

---

### Conclusi√≥n

La clasificaci√≥n es una herramienta poderosa en el aprendizaje autom√°tico y tiene numerosas aplicaciones en diversas √°reas. Los algoritmos como K-NN permiten que los modelos clasifiquen de manera precisa y eficiente, y las m√©tricas de evaluaci√≥n nos permiten medir y mejorar su desempe√±o.

---

## Decision Trees

Un **√°rbol de decisi√≥n** es un modelo predictivo utilizado en **clasificaci√≥n**. Es una estructura de √°rbol en la que cada nodo interno representa una prueba sobre un atributo, cada rama representa un resultado de la prueba y cada nodo hoja representa una clase o categor√≠a. Se utiliza para predecir el valor de un objetivo basado en caracter√≠sticas de entrada.

Imagina que eres un investigador m√©dico y necesitas predecir qu√© medicamento prescribir a un paciente, bas√°ndote en sus caracter√≠sticas, como edad, g√©nero, colesterol, etc. Los √°rboles de decisi√≥n pueden ayudarte a tomar esa decisi√≥n al clasificar al paciente en una de las categor√≠as, como "Medicamento A" o "Medicamento B".

---

## C√≥mo Construir un √Årbol de Decisi√≥n

### 1. **Elegir un Atributo para Dividir los Datos**

El primer paso es seleccionar un atributo para dividir el conjunto de datos. Los atributos pueden ser caracter√≠sticas como la **edad**, **g√©nero**, o **colesterol**. El objetivo es encontrar el atributo m√°s **predictivo** para dividir los datos de manera efectiva.

#### Ejemplo:
Supongamos que tenemos 14 pacientes y decidimos dividir los datos usando el atributo **g√©nero**. La divisi√≥n puede ser:
- **Mujeres**: mayor probabilidad de responder al medicamento A.
- **Hombres**: se necesita m√°s informaci√≥n para tomar una decisi√≥n.

### 2. **Dividir los Datos y Repetir el Proceso**

Despu√©s de hacer una primera divisi√≥n, repetimos el proceso para los subgrupos generados. Cada vez que dividimos los datos, buscamos hacer nodos m√°s **puros** (es decir, nodos donde la mayor√≠a de los casos pertenecen a una sola clase).

---

### Conceptos Clave

#### **Pureza de los Nodos**

Un nodo es **puro** cuando todos los datos en ese nodo pertenecen a la misma clase. El objetivo al construir un √°rbol de decisi√≥n es lograr la mayor pureza posible en los nodos, lo que ayuda a predecir de manera m√°s confiable.

#### **Entrop√≠a y Ganancia de Informaci√≥n**

- **Entrop√≠a**: Mide el desorden o incertidumbre en los datos. Un nodo con **entrop√≠a baja** es m√°s puro, mientras que un nodo con **entrop√≠a alta** es m√°s impuro.
- **Ganancia de Informaci√≥n**: Es la cantidad de certeza ganada al dividir los datos. Cuanto mayor sea la ganancia de informaci√≥n, mejor ser√° el atributo seleccionado para la divisi√≥n.

---

### Proceso de Construcci√≥n del √Årbol

#### 1. **Calcular la Entrop√≠a Inicial**
Primero calculamos la entrop√≠a de todo el conjunto de datos antes de hacer cualquier divisi√≥n. La f√≥rmula de la entrop√≠a nos da una medida de la incertidumbre en los datos. Cuanto m√°s equilibrada est√© la distribuci√≥n entre las clases, mayor ser√° la entrop√≠a.

#### 2. **Elegir el Mejor Atributo**
Seleccionamos el atributo que maximice la **ganancia de informaci√≥n**, es decir, el que reduzca m√°s la entrop√≠a. Este atributo ser√° el primero en dividir los datos.

**Ejemplo de C√°lculo de Entrop√≠a:**
Si tenemos 9 pacientes que responden al medicamento B y 5 que responden al medicamento A, la entrop√≠a antes de dividir ser√° de 0.94. Luego, calculamos la entrop√≠a para cada divisi√≥n posible y seleccionamos la que menor entrop√≠a tenga.

#### 3. **Repetir para Cada Rama**
Una vez divididos los datos, repetimos el proceso para cada subgrupo. Evaluamos la entrop√≠a en cada subgrupo y seleccionamos el mejor atributo para continuar dividiendo. Esto se hace recursivamente hasta que los nodos sean lo m√°s puros posible.

---

### Ejemplo Pr√°ctico: Predicci√≥n de Medicamento

Supongamos que tenemos un conjunto de datos con caracter√≠sticas como **edad**, **g√©nero** y **colesterol**. Queremos predecir qu√© medicamento debe recibir un nuevo paciente. 

1. Dividimos los pacientes por **g√©nero**: mujeres tienden a responder mejor al medicamento A, mientras que para los hombres necesitamos m√°s informaci√≥n.
2. Para los hombres, probamos con **colesterol**: si tienen colesterol alto, prescribimos el medicamento A; si es normal, prescribimos el medicamento B.

Este proceso sigue hasta que hemos clasificado a todos los pacientes en uno de los dos grupos (medicamento A o B).

---

### Conclusi√≥n

Construir un √°rbol de decisi√≥n consiste en dividir los datos de manera recursiva, seleccionando el atributo m√°s predictivo en cada paso para reducir la impureza de los nodos. A trav√©s de este proceso, podemos construir un modelo que prediga con precisi√≥n a qu√© clase pertenece un nuevo caso basado en sus caracter√≠sticas. 

La **ganancia de informaci√≥n** y la **entrop√≠a** son herramientas clave para seleccionar el mejor atributo en cada paso, asegurando que los nodos del √°rbol sean lo m√°s puros posible para hacer predicciones confiables.


## ¬øQu√© es la regresi√≥n log√≠stica?

La **regresi√≥n log√≠stica** es una t√©cnica estad√≠stica y de aprendizaje autom√°tico que se utiliza para clasificar registros de un conjunto de datos en funci√≥n de los valores de los campos de entrada. 

Imaginemos que tenemos un conjunto de datos de telecomunicaciones que nos gustar√≠a analizar para entender qu√© clientes podr√≠an abandonarnos el pr√≥ximo mes. Este es un conjunto de datos hist√≥rico de clientes, donde cada fila representa a un cliente. Como analistas de esta empresa, necesitamos identificar qui√©nes se van a marchar y por qu√©. Usaremos el conjunto de datos para construir un modelo basado en registros hist√≥ricos y predecir la p√©rdida futura de clientes.

El conjunto de datos incluye informaci√≥n sobre los servicios que cada cliente ha contratado, detalles de la cuenta del cliente, informaci√≥n demogr√°fica como g√©nero y edad, y tambi√©n informaci√≥n sobre aquellos clientes que han dejado la empresa en el √∫ltimo mes. A esta columna se le llama **churn** (abandono). 

Podemos usar la regresi√≥n log√≠stica para construir un modelo que prediga el abandono de clientes utilizando las caracter√≠sticas dadas. En la regresi√≥n log√≠stica, usamos una o m√°s variables independientes (como la antig√ºedad, la edad o los ingresos) para predecir un resultado, como el **abandono**, que es nuestra variable dependiente, representando si los clientes dejar√°n o no el servicio.

### Diferencias con la regresi√≥n lineal

La regresi√≥n log√≠stica es an√°loga a la **regresi√≥n lineal**, pero intenta predecir un campo de destino categ√≥rico o discreto, en lugar de uno num√©rico. En la regresi√≥n lineal, podr√≠amos intentar predecir un valor continuo, como el precio de una casa, la presi√≥n arterial de un paciente o el consumo de combustible de un coche. Sin embargo, en la regresi√≥n log√≠stica, predecimos una variable binaria, como "s√≠/no", "verdadero/falso", "exitoso/no exitoso", "embarazada/no embarazada", etc. Todos estos casos pueden ser codificados como 0 o 1.

Es importante destacar que las variables independientes en regresi√≥n log√≠stica deben ser continuas. Si son categ√≥ricas, deben ser codificadas como variables **dummy** o indicadores, lo que significa transformarlas en valores continuos.

### Aplicaciones de la regresi√≥n log√≠stica

Dado que la regresi√≥n log√≠stica es un tipo de algoritmo de clasificaci√≥n, se puede aplicar en diferentes situaciones. Algunos ejemplos incluyen:

- Predecir la probabilidad de que una persona sufra un ataque al coraz√≥n en un per√≠odo de tiempo determinado, bas√°ndonos en su edad, sexo y √≠ndice de masa corporal.
- Predecir la probabilidad de mortalidad en un paciente herido.
- Predecir si un paciente tiene una enfermedad como la diabetes, usando caracter√≠sticas observadas como peso, altura, presi√≥n arterial y resultados de diversas pruebas sangu√≠neas.
- En el contexto del marketing, predecir la probabilidad de que un cliente compre un producto o cancele una suscripci√≥n, como hicimos en el ejemplo de **churn**.
- Predecir la probabilidad de fallo de un proceso, sistema o producto.
- Predecir la probabilidad de que un propietario de vivienda incurra en un impago de hipoteca.

En todos estos ejemplos, no solo predecimos la clase de cada caso, sino que tambi√©n medimos la probabilidad de que un caso pertenezca a una clase espec√≠fica.

### ¬øCu√°ndo se debe usar regresi√≥n log√≠stica?

Existen varias situaciones en las que la regresi√≥n log√≠stica es una buena opci√≥n. Aqu√≠ presentamos cuatro de ellas:

1. **Cuando el campo objetivo (target) en los datos es categ√≥rico, espec√≠ficamente binario.** Esto puede ser algo como "0/1", "s√≠/no", "abandono/no abandono", "positivo/negativo", etc.
  
2. **Cuando necesitamos la probabilidad de nuestra predicci√≥n.** Por ejemplo, si queremos saber la probabilidad de que un cliente compre un producto. La regresi√≥n log√≠stica devuelve un puntaje de probabilidad entre 0 y 1 para una muestra de datos. De hecho, la regresi√≥n log√≠stica predice la probabilidad de esa muestra, y luego asigna la clase discreta a partir de esa probabilidad.

3. **Cuando los datos son linealmente separables.** La frontera de decisi√≥n de la regresi√≥n log√≠stica es una l√≠nea, un plano o un hiperplano. Un clasificador clasificar√° todos los puntos de un lado de la frontera de decisi√≥n en una clase y todos los del otro lado en la otra clase. Si tenemos solo dos caracter√≠sticas y no aplicamos ning√∫n procesamiento polin√≥mico, podemos obtener una desigualdad como \(\theta_0 + \theta_1 x_1 + \theta_2 x_2 > 0\), lo que resulta en un semiplano f√°cilmente representable gr√°ficamente. Es importante se√±alar que en la regresi√≥n log√≠stica tambi√©n podemos obtener una frontera de decisi√≥n compleja utilizando procesamiento polin√≥mico, aunque esto est√° fuera del alcance de esta explicaci√≥n.

4. **Cuando necesitamos entender el impacto de una caracter√≠stica.** Puedes seleccionar las mejores caracter√≠sticas bas√°ndote en la significancia estad√≠stica de los coeficientes o par√°metros del modelo de regresi√≥n log√≠stica. Es decir, despu√©s de encontrar los par√°metros √≥ptimos, una caracter√≠stica \(X\) con un peso \(\theta_1\) cercano a cero tiene un efecto menor sobre la predicci√≥n en comparaci√≥n con caracter√≠sticas con valores absolutos de \(\theta_1\) grandes. Esto nos permite entender el impacto de una variable independiente en la variable dependiente, controlando las dem√°s variables independientes.

### Formalizaci√≥n del problema

Para ilustrar c√≥mo funciona la regresi√≥n log√≠stica, volvamos a nuestro conjunto de datos. Definimos las variables independientes como **X** y la variable dependiente como **Y**. Para simplificar, podemos codificar los valores del objetivo o dependientes como 0 o 1. El objetivo de la regresi√≥n log√≠stica es construir un modelo que prediga la clase de cada muestra (en este caso, un cliente), as√≠ como la probabilidad de que cada muestra pertenezca a una clase.

Formalmente, tenemos:

- **X** es nuestro conjunto de datos en el espacio de n√∫meros reales, con **m** dimensiones o caracter√≠sticas y **n** registros.
- **Y** es la clase que queremos predecir, que puede ser 0 o 1.

Idealmente, un modelo de regresi√≥n log√≠stica, denotado como \(\hat{Y}\), predice que la clase del cliente es 1, dado sus caracter√≠sticas **X**. Tambi√©n se puede mostrar f√°cilmente que la probabilidad de que un cliente pertenezca a la clase 0 puede calcularse como \(1 - P(\text{clase 1})\).

## Diferencia entre regresi√≥n lineal y regresi√≥n log√≠stica

Para entender la regresi√≥n log√≠stica, es √∫til primero recordar c√≥mo funciona la regresi√≥n lineal. Imaginemos que queremos predecir el ingreso de los clientes de un conjunto de datos. En este caso, la variable independiente podr√≠a ser la edad del cliente, y la variable dependiente ser√≠a el ingreso, un valor continuo. Con regresi√≥n lineal, podemos ajustar una l√≠nea o polinomio a los datos y predecir un valor continuo para el ingreso de un cliente en funci√≥n de su edad.

Sin embargo, si intentamos predecir algo categ√≥rico, como la probabilidad de que un cliente abandone la empresa (churn), la regresi√≥n lineal no es apropiada. Para ilustrarlo, consideremos el problema de predecir el "churn" de los clientes, con una variable categ√≥rica: "churn s√≠" o "churn no". Si usamos regresi√≥n lineal para este caso, el modelo no proporcionar√≠a una probabilidad precisa de que un cliente pertenezca a una de las dos clases.

## El problema con la regresi√≥n lineal en clasificaci√≥n

En un gr√°fico, si usamos regresi√≥n lineal para predecir el "churn" de los clientes, los valores de salida pueden ser cualquier n√∫mero, como 3 o -2, que no tiene sentido en una clasificaci√≥n binaria. Para superar este problema, se utiliza un umbral para asignar las predicciones a una de las dos clases. Por ejemplo, si el valor predicho es mayor que 0.5, se asigna a la clase "churn s√≠" (1); si es menor, a la clase "churn no" (0).

El problema con este enfoque es que la regresi√≥n lineal no devuelve probabilidades de forma adecuada. La salida solo nos dice si el cliente pertenece a una clase u otra, pero no cu√°n probable es que eso ocurra. Adem√°s, la funci√≥n de activaci√≥n utilizada en la regresi√≥n lineal no refleja de manera precisa la probabilidad de un evento.

## Soluci√≥n: la funci√≥n sigmoide

Aqu√≠ es donde entra la regresi√≥n log√≠stica. En lugar de usar directamente el valor predicho por la regresi√≥n lineal, utilizamos una funci√≥n matem√°tica llamada **sigmoide**, que transforma el valor continuo en una probabilidad. Esta funci√≥n es conocida por su forma de "S" y asegura que la salida siempre est√© entre 0 y 1, lo que la hace adecuada para interpretar las predicciones como probabilidades.

La f√≥rmula de la funci√≥n sigmoide es:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

Donde \( z \) es el valor de la ecuaci√≥n de regresi√≥n lineal. Lo interesante de esta funci√≥n es que, cuando \( z \) es muy grande, la sigmoide se acerca a 1, lo que significa que la probabilidad de que el cliente pertenezca a la clase "churn s√≠" es alta. Si \( z \) es muy peque√±o, la sigmoide se acerca a 0, lo que indica una baja probabilidad de "churn s√≠".

## ¬øC√≥mo usamos la regresi√≥n log√≠stica?

La regresi√≥n log√≠stica modela la probabilidad de que un punto pertenezca a la clase predicha (por ejemplo, "churn s√≠"). Matem√°ticamente, podemos escribirlo como:

\[
P(y=1 \mid x) = \sigma(\theta^T x)
\]

Lo que nos dice es que la probabilidad de que el cliente pertenezca a la clase "churn s√≠" depende de las caracter√≠sticas \( x \) del cliente, como su edad e ingresos.

Por ejemplo, si la probabilidad de que un cliente abandone la empresa es del 80% dada su edad e ingresos, la f√≥rmula ser√≠a:

\[
P(\text{churn}=1 \mid \text{edad, ingresos}) = 0.8
\]

Y la probabilidad de que el cliente no abandone la empresa ser√≠a:

\[
P(\text{churn}=0 \mid \text{edad, ingresos}) = 1 - 0.8 = 0.2
\]

## Entrenamiento de un modelo de regresi√≥n log√≠stica

El objetivo del entrenamiento de la regresi√≥n log√≠stica es encontrar los valores adecuados para los par√°metros \( \theta \) que minimicen el error en nuestras predicciones. A continuaci√≥n, describimos los pasos generales para entrenar un modelo de regresi√≥n log√≠stica:

1. **Inicializaci√≥n de par√°metros**: Comenzamos con valores aleatorios para los par√°metros \( \theta \), por ejemplo, \( -1 \) o \( 2 \).
   
2. **C√°lculo de la salida del modelo**: Para cada cliente en el conjunto de entrenamiento, calculamos la salida del modelo, que es la probabilidad de que el cliente pertenezca a la clase 1 (por ejemplo, que haga "churn"). Esto se hace usando la funci√≥n sigmoide sobre el valor de \( \theta^T x \), que es la combinaci√≥n de los par√°metros y las caracter√≠sticas del cliente.
   
3. **Comparaci√≥n con el valor real**: Comparamos la salida del modelo con el valor real del cliente. Si el cliente realmente hizo "churn", la salida deber√≠a ser cercana a 1; si no hizo "churn", deber√≠a ser cercana a 0. Calculamos el error, que es la diferencia entre el valor real y el valor predicho.
   
4. **C√°lculo del error total**: Sumamos los errores de todos los clientes en el conjunto de entrenamiento. Esta suma es conocida como la "funci√≥n de costo", que representa el error total del modelo. El objetivo es minimizar esta funci√≥n de costo.
   
5. **Actualizaci√≥n de los par√°metros**: Usamos un algoritmo de optimizaci√≥n (como el **descenso por gradiente**) para ajustar los par√°metros \( \theta \) y reducir el error.

6. **Iteraci√≥n**: Continuamos el proceso de c√°lculo y actualizaci√≥n de los par√°metros hasta que el modelo tenga un error suficientemente bajo.

## ¬øC√≥mo decidimos cu√°ndo detener el entrenamiento?

Existen diferentes criterios para detener el entrenamiento. Uno de los m√°s comunes es observar la **precisi√≥n del modelo**. Si la precisi√≥n del modelo es lo suficientemente alta y no mejora significativamente con m√°s iteraciones, detenemos el entrenamiento.

## Conclusi√≥n

En resumen, la regresi√≥n log√≠stica es una t√©cnica poderosa para resolver problemas de clasificaci√≥n binaria, ya que no solo predice la clase de un punto, sino tambi√©n la probabilidad de que pertenezca a esa clase. A trav√©s del entrenamiento del modelo y el uso de la funci√≥n sigmoide, podemos construir modelos que no solo predicen correctamente la clase, sino que tambi√©n nos dan una estimaci√≥n precisa de la probabilidad asociada con cada predicci√≥n.

## Objetivo del Entrenamiento en Regresi√≥n Log√≠stica

El objetivo principal del entrenamiento en regresi√≥n log√≠stica es cambiar los par√°metros del modelo para que sea la mejor estimaci√≥n de las etiquetas de las muestras en el conjunto de datos, como por ejemplo, el churn de los clientes. 

¬øC√≥mo hacemos esto? En resumen, primero tenemos que observar la funci√≥n de costo y ver cu√°l es la relaci√≥n entre la funci√≥n de costo y los par√°metros \( \theta \). Entonces, debemos formular la funci√≥n de costo. Luego, utilizando la derivada de la funci√≥n de costo, podemos encontrar c√≥mo cambiar los par√°metros para reducir el costo o el error.

**Nota importante**: Para entender este proceso, es necesario contar con algunos conocimientos matem√°ticos b√°sicos. Sin embargo, no te preocupes, ya que la mayor√≠a de los lenguajes de ciencia de datos como Python, R y Scala tienen paquetes o bibliotecas que calculan estos par√°metros por ti.

## La Funci√≥n de Costo

Vamos a ver c√≥mo encontramos la ecuaci√≥n de la funci√≥n de costo para un caso de ejemplo. Para hacerlo, podemos usar uno de los clientes del problema de churn.

En general, la ecuaci√≥n para calcular el costo es la diferencia entre los valores reales de \( y \) y nuestra salida del modelo \( \hat{y} \). Esta es una regla general para la mayor√≠a de las funciones de costo en machine learning. Podemos mostrar esto como el costo de nuestro modelo comparado con las etiquetas reales, lo cual es la diferencia entre el valor predicho por nuestro modelo y el valor real del campo objetivo, donde el valor predicho de nuestro modelo es la sigmoide de \( \theta^T x \).

Por lo general, se usa el cuadrado de esta ecuaci√≥n debido a la posibilidad de obtener un resultado negativo, y por simplicidad, se considera la mitad de este valor como la funci√≥n de costo durante el proceso de derivada.

Ahora, podemos escribir la funci√≥n de costo para todas las muestras en nuestro conjunto de entrenamiento. Por ejemplo, para todos los clientes podemos escribirla como la suma promedio de las funciones de costo de todos los casos. Esta es conocida como el error cuadr√°tico medio y, como es una funci√≥n de un vector de par√°metros \( \theta \), se muestra como \( J(\theta) \).

## Encontrando los Mejores Par√°metros

Ahora que tenemos la funci√≥n de costo, ¬øc√≥mo encontramos o establecemos los mejores pesos o par√°metros que minimizan esta funci√≥n de costo? La respuesta es que debemos calcular el punto m√≠nimo de esta funci√≥n de costo, lo cual nos mostrar√° los mejores par√°metros para nuestro modelo.

Aunque podemos encontrar el punto m√≠nimo de una funci√≥n utilizando la derivada de la funci√≥n, no existe una manera f√°cil de encontrar el m√≠nimo global para una ecuaci√≥n como esta. Dado que esto es complicado, describir c√≥mo llegar al m√≠nimo global est√° fuera del alcance de este video.

### Soluci√≥n: Usar la Funci√≥n Logaritmo Negativo

La soluci√≥n es encontrar otra funci√≥n de costo que tenga el mismo comportamiento pero que sea m√°s f√°cil de minimizar. 

Para ilustrarlo, recordemos que nuestro modelo es \( \hat{y} \), y nuestro valor real es \( y \), que puede ser 0 o 1. Queremos encontrar una funci√≥n de costo simple para nuestro modelo. Supongamos que el valor deseado para \( y \) es 1. Esto significa que nuestro modelo es mejor si estima que \( y = 1 \). 

En este caso, necesitamos una funci√≥n de costo que devuelva 0 si el resultado de nuestro modelo es 1, lo cual coincide con la etiqueta real. La funci√≥n de costo debe aumentar a medida que el resultado de nuestro modelo se aleja de 1, y debe ser muy grande si el resultado de nuestro modelo se acerca a 0. La funci√≥n logaritmo negativo proporciona una funci√≥n de costo como la que necesitamos.

Esto significa que si el valor real es 1 y el modelo tambi√©n predice 1, la funci√≥n logaritmo negativo devuelve un costo de 0. Pero si la predicci√≥n es menor que 1, la funci√≥n logaritmo negativo devuelve un costo mayor.

### Funci√≥n de Costo para Regresi√≥n Log√≠stica

As√≠ que podemos usar la funci√≥n logaritmo negativo para calcular el costo de nuestro modelo de regresi√≥n log√≠stica. Si recordamos, previamente dijimos que generalmente es dif√≠cil calcular la derivada de la funci√≥n de costo. Ahora podemos cambiarla por el logaritmo negativo de nuestro modelo.

Podemos demostrar f√°cilmente que, si el valor deseado \( y \) es 1, el costo se puede calcular como \( -\log(\hat{y}) \), y si el valor deseado \( y \) es 0, el costo se puede calcular como \( -\log(1 - \hat{y}) \).

Ahora podemos sustituir esto en nuestra funci√≥n de costo total y reescribirla como esta funci√≥n. Esta es la funci√≥n de costo para la regresi√≥n log√≠stica. Como puedes ver, penaliza situaciones en las que la clase es 0 y la salida del modelo es 1, y viceversa. Recuerda, sin embargo, que \( \hat{y} \) no devuelve una clase como salida, sino un valor entre 0 y 1 que debe ser interpretado como una probabilidad.

## Minimizando la Funci√≥n de Costo: Usando el Descenso por Gradiente

Ahora podemos usar esta funci√≥n para encontrar los par√°metros de nuestro modelo de tal forma que minimicen el costo. 

**Recapitulando**: Nuestro objetivo era encontrar un modelo que estimara mejor las etiquetas reales. Encontrar el mejor modelo significa encontrar los mejores par√°metros \( \theta \) para ese modelo. La primera pregunta era, ¬øc√≥mo encontrar los mejores par√°metros para nuestro modelo? La respuesta es minimizando la funci√≥n de costo de nuestro modelo. En otras palabras, para minimizar \( J(\theta) \), acabamos de definir.

### ¬øQu√© es el Descenso por Gradiente?

La siguiente pregunta es, ¬øc√≥mo minimizamos la funci√≥n de costo? La respuesta es utilizando un enfoque de optimizaci√≥n. Hay diferentes enfoques de optimizaci√≥n, pero usamos uno de los m√°s famosos y efectivos: el **descenso por gradiente**.

El descenso por gradiente es un enfoque iterativo para encontrar el m√≠nimo de una funci√≥n. Espec√≠ficamente, en nuestro caso, el descenso por gradiente es una t√©cnica que utiliza la derivada de una funci√≥n de costo para cambiar los valores de los par√°metros y minimizar el costo o error.

### Funcionamiento del Descenso por Gradiente

El objetivo principal del descenso por gradiente es cambiar los valores de los par√°metros para minimizar el costo. ¬øC√≥mo puede el descenso por gradiente hacer esto? Imagina que los par√°metros o pesos de nuestro modelo est√°n en un espacio bidimensional. Por ejemplo, \( \theta_1 \) y \( \theta_2 \) para dos conjuntos de caracter√≠sticas, edad e ingresos.

Queremos minimizar la funci√≥n de costo \( J \), que es una funci√≥n de las variables \( \theta_1 \) y \( \theta_2 \). Si graficamos la funci√≥n de costo en funci√≥n de todos los posibles valores de \( \theta_1 \) y \( \theta_2 \), podemos ver algo como esto:

- Esto representa el valor del error para diferentes valores de los par√°metros, es decir, el error en funci√≥n de los par√°metros. A esto se le llama la curva de error o el "cuenco de error" de la funci√≥n de costo.

Queremos usar este cuenco de error para encontrar los mejores valores de par√°metros que minimicen el costo.

### Actualizaci√≥n de Par√°metros

Para minimizar el costo, debemos calcular la derivada parcial de la funci√≥n de costo \( J \) con respecto a cada par√°metro en ese punto. Esta derivada nos da la pendiente de la superficie en ese punto y la direcci√≥n en la que debemos movernos para reducir el error. Moverse en la direcci√≥n opuesta a la pendiente garantiza que descendemos en la curva de error.

Los valores de la derivada tambi√©n nos indican el tama√±o de los pasos a tomar: si la pendiente es grande, tomamos un paso grande, porque estamos lejos del m√≠nimo; si la pendiente es peque√±a, tomamos un paso m√°s peque√±o.

### El Proceso Iterativo

El descenso por gradiente es un proceso iterativo: en cada iteraci√≥n, actualizamos los par√°metros y minimizamos el costo hasta que el algoritmo converge a un m√≠nimo aceptable.

## Resumen del Algoritmo de Entrenamiento

Recapitulemos lo que hemos hecho hasta ahora, paso a paso:

1. Inicializamos los par√°metros con valores aleatorios.
2. Alimentamos la funci√≥n de costo con el conjunto de entrenamiento y calculamos el costo. Esperamos una tasa de error alta, ya que los par√°metros est√°n establecidos aleatoriamente.
3. Calculamos el gradiente de la funci√≥n de costo utilizando la derivada parcial.
4. Actualizamos los pesos con los nuevos valores de par√°metros.
5. Volvemos al paso 2 y alimentamos la funci√≥n de costo con los nuevos par√°metros. Esperamos que el error disminuya.
6. Continuamos este ciclo hasta alcanzar un valor de costo bajo o un n√∫mero limitado de iteraciones.

Finalmente, despu√©s de algunas iteraciones, los par√°metros deber√≠an estar aproximadamente ajustados, lo que significa que el modelo est√° listo para predecir la probabilidad de que un cliente se quede o se vaya.


## Algoritmo de M√°quinas de Vectores de Soporte (SVM)

**Support Vector Machine (SVM)**, un algoritmo supervisado utilizado para la **clasificaci√≥n** de datos, especialmente eficaz en el an√°lisis de datos de alta dimensi√≥n.

### 1. ¬øQu√© es SVM?
- SVM es un algoritmo **supervisado** utilizado para **clasificaci√≥n**. 
- Se basa en encontrar un **hiperplano** que separe las clases de datos en un espacio de caracter√≠sticas.
- Inicialmente, los datos son transformados a un espacio de caracter√≠sticas de mayor dimensi√≥n, donde es m√°s f√°cil separar las clases mediante un hiperplano.
- **Transformaci√≥n de datos**: Se realiza a trav√©s de una t√©cnica llamada **kernel**, que mapea los datos de un espacio de baja dimensi√≥n a uno de mayor dimensi√≥n, donde los datos se vuelven **linealmente separables**.

### 2. Transformaci√≥n de los datos (Kernelling)
- **Kernelling** es el proceso de mapear los datos a un espacio de mayor dimensi√≥n para hacer que sean linealmente separables.
- Esto se logra mediante una **funci√≥n kernel** (como lineal, polin√≥mica, RBF o sigmoide), cuya elecci√≥n depende de los datos y se puede probar experimentalmente.
- **Ejemplo**: Si los datos no son linealmente separables en un espacio 1D, se pueden mapear a un espacio 2D donde la separaci√≥n se vuelve posible.

### 3. B√∫squeda del hiperplano √≥ptimo
- El objetivo de SVM es encontrar el **hiperplano que maximiza la separaci√≥n entre las dos clases** (mayor margen).
- **Soportes vectores**: Los puntos m√°s cercanos al hiperplano que son los m√°s importantes para determinar la ubicaci√≥n del hiperplano. Otros puntos no son tan relevantes.
- El proceso de **optimizaci√≥n** busca el hiperplano que maximiza la distancia al **soporte vectorial**, es decir, el margen entre las dos clases.
- Esta optimizaci√≥n se puede resolver con **descenso de gradiente**, aunque los detalles matem√°ticos no se abordan en este video.

### 4. Clasificaci√≥n con el hiperplano
- Una vez entrenado el modelo, para **clasificar nuevos puntos**, se utiliza la ecuaci√≥n del hiperplano.
- Si el resultado de la ecuaci√≥n es mayor que 0, el punto se clasifica en la clase superior (por encima del hiperplano); si es menor, en la clase inferior (debajo del hiperplano).

### 5. Ventajas y desventajas de SVM

#### Ventajas:
- Es **preciso** en **espacios de alta dimensi√≥n**.
- Utiliza solo un subconjunto de los puntos de entrenamiento llamados **soportes vectores**, lo que lo hace **eficiente en memoria**.

#### Desventajas:
- Puede **sobreajustarse** si el n√∫mero de caracter√≠sticas es mucho mayor que el n√∫mero de muestras.
- No proporciona directamente **estimaciones de probabilidad**, lo cual es necesario en muchos problemas de clasificaci√≥n.
- **No es eficiente** computacionalmente con datasets muy grandes (m√°s de 1,000 muestras).

### 6. ¬øCu√°ndo utilizar SVM?
- **An√°lisis de im√°genes**: clasificaci√≥n de im√°genes, reconocimiento de d√≠gitos manuscritos.
- **Miner√≠a de textos**: detecci√≥n de spam, an√°lisis de sentimientos, clasificaci√≥n de texto.
- **Clasificaci√≥n de datos de expresi√≥n g√©nica**: debido a la efectividad en datos de alta dimensi√≥n.
- Tambi√©n se puede usar en **regresi√≥n**, **detecci√≥n de anomal√≠as** y **agrupamiento**.

En resumen, SVM es un potente algoritmo para clasificaci√≥n, especialmente cuando se trabaja con datos de alta dimensi√≥n, pero tiene algunas limitaciones, especialmente cuando los datos son muy grandes o el sobreajuste es un problema.

---

## Predicci√≥n Multiclase: SoftMax, Uno-contra-Todo y Uno-contra-Uno en Clasificaci√≥n Multiclase

En clasificaci√≥n multiclase, el objetivo es clasificar datos en m√∫ltiples etiquetas de clase. A diferencia de m√©todos como √°rboles de decisi√≥n o vecinos cercanos, la clasificaci√≥n multiclase en clasificadores lineales no es tan directa. Podemos extender la regresi√≥n log√≠stica a una clasificaci√≥n multiclase usando **regresi√≥n SoftMax**, que es una generalizaci√≥n de la regresi√≥n log√≠stica. Sin embargo, la regresi√≥n SoftMax no funciona con m√°quinas de vectores soporte (SVM); en su lugar, podemos usar los m√©todos **Uno-contra-Todo** y **Uno-contra-Uno**, que adaptan clasificadores de dos clases para trabajar en problemas multiclase.

### Regresi√≥n SoftMax

La regresi√≥n SoftMax es similar a la regresi√≥n log√≠stica, ya que transforma los resultados obtenidos de multiplicar los datos \( x \) con los par√°metros \( \theta \) en probabilidades. Para una clasificaci√≥n de \( K \) clases (0 a \( K-1 \)), se aplica la funci√≥n SoftMax, que convierte estas distancias en probabilidades usando la f√≥rmula:

\[
\text{softmax}(x, i) = \frac{e^{-\theta_i^T x}}{\sum_{j=1}^{K} e^{-\theta_j^T x}}
\]

El entrenamiento en regresi√≥n SoftMax es casi id√©ntico al de la regresi√≥n log√≠stica, usando la entrop√≠a cruzada. Para hacer predicciones, se calcula la probabilidad de pertenencia a cada clase y se elige la de mayor probabilidad con la funci√≥n de m√°ximo:

\[
\hat{y} = \text{argmax}_i(\text{softmax}(x, i))
\]

Por ejemplo, para un dato \( x_1 \) y tres clases, podr√≠amos obtener las siguientes probabilidades:

| Clase | Probabilidad |
|-------|--------------|
| 0     | 0.97         |
| 1     | 0.02         |
| 2     | 0.01         |

La predicci√≥n ser√≠a la clase con mayor probabilidad, en este caso, **clase 0**.

#### Interpretaci√≥n Geom√©trica

Cada par√°metro \( \theta_i^T x \) representa un hiperplano en el espacio de caracter√≠sticas. La intersecci√≥n de estos hiperplanos crea regiones en las que cada regi√≥n representa una clase. Por ejemplo, una entrada en la regi√≥n azul se clasificar√≠a como clase 0, una en la regi√≥n roja como clase 1, y una en la regi√≥n amarilla como clase 2.

**Nota**: La regresi√≥n SoftMax no es compatible con SVM, ya que el entrenamiento en SVM se basa en la maximizaci√≥n del margen y no en la entrop√≠a cruzada.

### Uno-contra-Todo (One-vs-Rest)

En el m√©todo Uno-contra-Todo, si hay \( K \) clases, se crean \( K \) clasificadores binarios. Para cada clasificador, los ejemplos de la clase de inter√©s se mantienen como est√°n, y los ejemplos de las dem√°s clases se agrupan en una "clase dummy". As√≠, cada clasificador aprende a diferenciar su clase de inter√©s contra el resto.

Para hacer una predicci√≥n, se ignoran las "clases dummy" y se selecciona la clase que obtuvo el mayor valor en su clasificador correspondiente.

#### Problema de Ambig√ºedad en Uno-contra-Todo

En algunas √°reas de espacio, varios clasificadores pueden dar respuestas conflictivas. Para mitigar estas regiones ambiguas, se pueden usar reglas de combinaci√≥n (como elegir la clase con el puntaje m√°s alto) o considerar la probabilidad de pertenencia real de cada muestra.

### Uno-contra-Uno (One-vs-One)

En el m√©todo Uno-contra-Uno, se entrena un clasificador binario para cada par de clases posibles. Por ejemplo, si hay clases 0, 1 y 2, se entrena un clasificador para distinguir entre 0 y 1, otro para 0 y 2, y uno m√°s para 1 y 2. 

Para \( K \) clases, este m√©todo requiere \( K(K-1)/2 \) clasificadores binarios. Para hacer una predicci√≥n en un nuevo dato, se toma una "votaci√≥n por mayor√≠a" entre los clasificadores. La clase con m√°s votos es la elegida. Aunque tambi√©n puede haber √°reas de ambig√ºedad, estas son m√°s peque√±as que en el m√©todo Uno-contra-Todo.

### Resumen

- **Regresi√≥n SoftMax**: Transforma regresi√≥n log√≠stica para clasificaci√≥n multiclase usando una funci√≥n de probabilidad. No es compatible con SVM.
- **Uno-contra-Todo**: Divide el problema en m√∫ltiples clasificadores binarios (uno para cada clase vs. todas las dem√°s). Puede generar √°reas ambiguas.
- **Uno-contra-Uno**: Divide el problema en clasificadores para cada par de clases, reduciendo las √°reas de ambig√ºedad con votaci√≥n por mayor√≠a.

---

## Introducci√≥n a Clustering y sus Aplicaciones

### 1. ¬øQu√© es el Clustering?
El **clustering** es un enfoque no supervisado utilizado para agrupar datos en funci√≥n de su similitud. Un ejemplo de aplicaci√≥n es la **segmentaci√≥n de clientes**:
- **Segmentaci√≥n de clientes**: La pr√°ctica de dividir una base de clientes en grupos de individuos con caracter√≠sticas similares, lo que permite a las empresas asignar recursos de marketing de manera m√°s eficiente.
    - Ejemplo de grupos: 
        - Clientes de alto beneficio y bajo riesgo.
        - Clientes de organizaciones sin fines de lucro.
- **Proceso general**: El clustering permite **segmentar grandes vol√∫menes de datos** con caracter√≠sticas como edad, g√©nero, intereses, h√°bitos de compra, etc., para identificar c√≥mo los clientes son similares entre s√≠.

### 2. ¬øC√≥mo Funciona el Clustering?
El **clustering** agrupa datos en funci√≥n de la similitud de los puntos, sin supervisi√≥n, dividiendo el conjunto de datos en **grupos mutuamente excluyentes**.
- Ejemplo: Un conjunto de clientes puede ser segmentado en 3 **clusters**:
    1. Clientes acomodados de mediana edad.
    2. J√≥venes, educados y de ingresos medios.
    3. J√≥venes y de bajos ingresos.
- **Aplicaci√≥n**: Cruzando este conjunto segmentado con datos sobre productos/servicios comprados, se puede entender y predecir las preferencias de compra y comportamientos de los clientes.

### 3. Clustering vs. Clasificaci√≥n
La **clasificaci√≥n** asigna etiquetas a los datos, es un enfoque supervisado, mientras que el **clustering** agrupa los datos sin etiquetas predefinidas.
- **Clasificaci√≥n**: Se usa para predecir etiquetas categ√≥ricas (ej. cliente que va a incurrir en impago).
- **Clustering**: Es un enfoque no supervisado, donde los datos no tienen etiquetas y el algoritmo agrupa instancias similares, por ejemplo, a trav√©s de algoritmos como **K-Means**.

### 4. Aplicaciones del Clustering
El clustering tiene muchas aplicaciones en diferentes sectores:
- **Retail**: Para encontrar asociaciones entre clientes seg√∫n sus caracter√≠sticas demogr√°ficas y patrones de compra.
- **Sistemas de recomendaci√≥n**: Para identificar grupos de usuarios similares y realizar filtrado colaborativo, recomendando productos como libros o pel√≠culas.
- **Banca**: Para detectar patrones de uso fraudulento de tarjetas de cr√©dito y clasificar clientes (ej. leales vs. clientes perdidos).
- **Seguros**: Para detectar fraudes en reclamaciones o evaluar el riesgo de seguros de clientes seg√∫n sus segmentos.
- **Medios de publicaci√≥n**: Para categorizar autom√°ticamente noticias y recomendar art√≠culos similares a los lectores.
- **Medicina**: Para caracterizar el comportamiento de pacientes seg√∫n caracter√≠sticas similares y encontrar terapias exitosas.
- **Biolog√≠a**: Para agrupar genes con patrones de expresi√≥n similares o identificar relaciones familiares a trav√©s de marcadores gen√©ticos.

### 5. Usos del Clustering
Generalmente, el clustering se puede usar para:
- **An√°lisis exploratorio de datos**.
- **Generaci√≥n de res√∫menes** o reducci√≥n de escala.
- **Detecci√≥n de anomal√≠as**, especialmente para fraude o eliminaci√≥n de ruido.
- **Identificaci√≥n de duplicados** en conjuntos de datos.
- Como un paso previo en tareas de **predicci√≥n**, **miner√≠a de datos** o **sistemas complejos**.

### 6. Tipos de Algoritmos de Clustering

#### 6.1. Clustering Basado en Particiones
- Produce clusters de forma esf√©rica.
- Ejemplos: **K-Means**, **K-Medians**, **Fuzzy c-Means**.
- Usados para bases de datos medianas y grandes.

#### 6.2. Clustering Jer√°rquico
- Produce √°rboles de clusters.
- Ejemplos: Algoritmos **aglomerativos** y **divisivos**.
- Son intuitivos y buenos para bases de datos peque√±as.

#### 6.3. Clustering Basado en Densidad
- Produce clusters con formas arbitrarias.
- Especialmente √∫tiles para manejar clusters espaciales o datos con ruido.
- Ejemplo: **DBSCAN**.

---
## K-Means Clustering

### 1. Introducci√≥n
- **K-Means Clustering** es un algoritmo de **clustering no supervisado**.
- Se utiliza para agrupar datos bas√°ndose en la similitud entre los puntos de datos.
- Un ejemplo com√∫n de uso es la **segmentaci√≥n de clientes**, donde el objetivo es dividir a los clientes en grupos con caracter√≠sticas similares.

**Objetivo de K-Means:**
- Formar **K clusters** de manera que los puntos dentro de un mismo cluster sean lo m√°s similares posible y los puntos de diferentes clusters lo m√°s distintos posible.

### 2. ¬øC√≥mo se mide la similitud?
- Se usan m√©tricas de **distancia** para medir la similitud entre los puntos de datos.
- La **distancia Euclidiana** es la m√°s com√∫nmente usada, especialmente en datos multidimensionales.

**Alternativas:**
- **Coseno de similitud**.
- **Distancia promedio**.

### 3. ¬øC√≥mo funciona K-Means?

#### Pasos del algoritmo:

1. **Inicializaci√≥n de centroides**:
   - Colocar aleatoriamente **K centroides** en el espacio de datos.

2. **C√°lculo de distancias**:
   - Calcular la distancia de cada punto de datos a los centroides utilizando **distancia Euclidiana** (u otras m√©tricas si se requiere).

3. **Asignaci√≥n de puntos**:
   - Asignar cada punto al centroide m√°s cercano, formando clusters.

4. **Recalcular los centroides**:
   - Despu√©s de asignar los puntos, recalcular la posici√≥n de los centroides como el promedio de todos los puntos dentro de cada cluster.

5. **Repetir el proceso**:
   - Continuar iterando hasta que los centroides ya no se desplacen.

#### Convergencia del algoritmo:
- El algoritmo es **iterativo** y continuar√° moviendo los centroides y reasignando puntos hasta que los centroides se estabilicen.
- El objetivo es **minimizar la distancia** entre los puntos de datos y sus centroides, pero puede llegar a un **√≥ptimo local**.
- Para evitar √≥ptimos locales, se recomienda ejecutar el algoritmo m√∫ltiples veces con diferentes inicializaciones.

### 4. Evaluaci√≥n de la Calidad de los Clusters

#### ¬øC√≥mo evaluar la bondad de los clusters?
1. **Comparaci√≥n con la verdad de terreno** (si est√° disponible).
2. **C√°lculo del error dentro del cluster**:
   - Medir la **distancia promedio** entre los puntos de datos y los centroides.

### 5. Determinaci√≥n del N√∫mero de Clusters (K)

#### M√©todo del Codo
- Para elegir el valor adecuado de **K**, se grafica la **distancia entre los puntos de datos y sus centroides** a medida que var√≠a K.
- A medida que aumenta K, la distancia disminuye, pero llega un punto donde la disminuci√≥n se desacelera. Este punto es conocido como **el punto de codo** y se√±ala el n√∫mero √≥ptimo de clusters.

#### M√©todos para encontrar el mejor K:
- Calcular la **distancia promedio** entre los puntos y los centroides, lo que indica qu√© tan densos son los clusters.
- Observar c√≥mo cambia esta m√©trica al aumentar K, buscando el punto donde la tasa de disminuci√≥n cambia abruptamente.

### 6. Resumen de K-Means

- **K-Means** es un algoritmo **basado en particiones** que es **eficiente** en conjuntos de datos medianos y grandes.
- Los clusters suelen tener una **forma esf√©rica** debido a la distribuci√≥n alrededor de los centroides.
- **Desventaja**: Se debe predefinir el n√∫mero de clusters (**K**), lo que puede ser complicado si no se tiene conocimiento previo de los datos.

**Aplicaciones comunes de K-Means:**
- Segmentaci√≥n de clientes.
- An√°lisis de patrones de compra.
- Identificaci√≥n de fraudes.
- Sistemas de recomendaci√≥n.

