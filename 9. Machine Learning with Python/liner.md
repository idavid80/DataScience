# Regresi√≥n Lineal

Existen dos tipos b√°sicos de modelos de regresi√≥n lineal:
- **Regresi√≥n Lineal Simple**: Aqu√≠ se utiliza una sola variable independiente para predecir una variable dependiente. Por ejemplo, si quisi√©ramos estimar las emisiones de CO‚ÇÇ de un autom√≥vil, podr√≠amos usar como √∫nico factor el tama√±o del motor.
- **Regresi√≥n Lineal M√∫ltiple**: Este tipo de regresi√≥n incluye varias variables independientes para mejorar la precisi√≥n de las predicciones. Por ejemplo, para predecir las emisiones de CO‚ÇÇ de un autom√≥vil, adem√°s del tama√±o del motor, tambi√©n se pueden considerar el n√∫mero de cilindros y el consumo de combustible.

## ¬øPara Qu√© Sirve la Regresi√≥n Lineal M√∫ltiple?
Este modelo es √∫til para dos tipos de aplicaciones:
1. **Evaluar el impacto de las variables independientes en la variable dependiente**: Por ejemplo, podemos analizar c√≥mo variables como el tiempo de estudio, la asistencia a clases y el g√©nero afectan el rendimiento de un estudiante en un examen.
2. **Predecir cambios en la variable dependiente**: Nos permite prever c√≥mo cambiar√° la variable dependiente si modificamos alguna de las variables independientes. Por ejemplo, podr√≠amos estimar cu√°nto aumentar√° la presi√≥n arterial de una persona al incrementar su √≠ndice de masa corporal, manteniendo constantes otros factores.

## La Ecuaci√≥n de Regresi√≥n Lineal M√∫ltiple
En un modelo de regresi√≥n lineal m√∫ltiple, la variable objetivo **Y** se representa como una combinaci√≥n lineal de las variables independientes (o predictoras), a las que llamaremos **X‚ÇÅ, X‚ÇÇ, X‚ÇÉ**, y as√≠ sucesivamente. La ecuaci√≥n tiene la forma:

\[
\hat{y} = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \dots + \theta_n X_n
\]

Donde:
- **ùúÉ‚ÇÄ** es el intercepto (o constante de ajuste),
- **ùúÉ‚ÇÅ, ùúÉ‚ÇÇ, \dots, ùúÉ‚Çô** son los coeficientes que muestran el peso o la importancia de cada variable en la predicci√≥n de **Y**.

En pocas palabras, la ecuaci√≥n busca estimar los valores de estos coeficientes (**ùúÉ**) para que el modelo ajuste lo mejor posible los datos y minimice el error en las predicciones.

## C√≥mo Encontrar los Mejores Coeficientes (Optimizaci√≥n de Par√°metros)
La optimizaci√≥n de los coeficientes **ùúÉ** se realiza buscando minimizar el error de las predicciones. Hay dos m√©todos comunes:
- **M√≠nimos Cuadrados Ordinarios (OLS)**: Es una t√©cnica que minimiza el error cuadr√°tico medio, √∫til cuando el tama√±o del conjunto de datos es manejable (menos de 10,000 filas).
- **Algoritmos de Optimizaci√≥n**: Como el descenso por gradiente, que es eficaz para grandes conjuntos de datos. Este m√©todo comienza con valores aleatorios para los coeficientes y los ajusta iterativamente para reducir el error.

## Evaluaci√≥n del Modelo: ¬øQu√© Tan Bien Predice?
Para evaluar el modelo, se calcula el error promedio de las predicciones con el **Error Cuadr√°tico Medio (MSE)**. Un menor valor de MSE indica que el modelo est√° ajustando mejor los datos.

## Predicci√≥n con el Modelo
Una vez optimizados los coeficientes, la predicci√≥n es directa: solo se sustituyen los valores de las variables independientes en la ecuaci√≥n para obtener el valor estimado de la variable objetivo.

Por ejemplo, si en el caso de emisiones de CO‚ÇÇ encontramos que:

\[
Emisiones\ de\ CO‚ÇÇ = 125 + (6.2 \times Tama√±o\ del\ Motor) + (14 \times N√∫mero\ de\ Cilindros)
\]

Podr√≠amos calcular el valor de emisiones para un autom√≥vil espec√≠fico insertando sus datos de tama√±o de motor y cilindros en esta ecuaci√≥n.

## Consideraciones Adicionales
- **N√∫mero de Variables**: Incluir muchas variables sin justificaci√≥n te√≥rica puede generar un modelo sobreajustado (overfitting), que no generaliza bien para otros datos. Se recomienda seleccionar solo
